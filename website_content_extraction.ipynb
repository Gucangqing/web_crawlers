{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_chinese_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Determine the encoding of the response\n",
    "    encoding = 'utf-8'\n",
    "    \n",
    "    # Create a Beautiful Soup object\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #print(soup)\n",
    "    # Find all HTML elements that contain Chinese content\n",
    "    chinese_content = soup.find_all(string=lambda text: text and any(characters >= u'\\u4e00' and characters <= u'\\u9fff' for characters in text))\n",
    "    \n",
    "    # Print or process the extracted Chinese content\n",
    "    #for content in chinese_content:\n",
    "        #print(content)\n",
    "\n",
    "    return chinese_content\n",
    "#extract_chinese_content('https://www.rfi.fr/cn/%E4%B8%AD%E5%9B%BD/20121213-%E5%9B%9B%E5%B7%9D%E7%9C%81%E5%A7%94%E5%89%AF%E4%B9%A6%E8%AE%B0%E6%9D%8E%E6%98%A5%E5%9F%8E%E5%8D%96%E7%88%B5%E9%AC%BB%E5%AE%98%E9%81%AD%E5%AE%9E%E5%90%8D%E4%B8%BE%E6%8A%A5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_english_content(input_string):\n",
    "    # Regular expression pattern to match English content\n",
    "    pattern = r'[a-zA-Z,.\\'’\"“”‘]+'\n",
    "    \n",
    "    # Replace English content with an empty string\n",
    "    result = re.sub(pattern, '', input_string)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_excel(input_file, output_file):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Create a new list to store the processed data\n",
    "    processed_data = []\n",
    "    count = 0\n",
    "    columns=df.columns\n",
    "    # Iterate over the columns\n",
    "    for i in range(0,len(columns)):\n",
    "        count += 1\n",
    "        \n",
    "        # Get the column data\n",
    "        column_data = df[columns[i]]\n",
    "        \n",
    "        # Iterate over the website links in the column\n",
    "        for website in column_data:\n",
    "            print(website)\n",
    "            if isinstance(website, str):\n",
    "                # Process the website link using the 'process' function\n",
    "                try:\n",
    "                    content = extract_chinese_content(website)  # Replace 'process' with your actual processing function\n",
    "                    \n",
    "                    # Create a new row for the processed data\n",
    "                    new_row = pd.DataFrame({'NAME': [columns[i]],\n",
    "                                            'WEBSITE': [website],\n",
    "                                            'CONTENT': [content]})\n",
    "                    \n",
    "                    # Append the new row to the processed data list\n",
    "                    processed_data.append(new_row)\n",
    "                except:\n",
    "                    continue\n",
    "        print('------------output',count,'------------')\n",
    "            # Concatenate the processed data list every 10 iterations and save to CSV\n",
    "        processed_data_concatenated = pd.concat(processed_data, ignore_index=True)\n",
    "        processed_data_concatenated.to_csv(output_file, index=False)\n",
    "            \n",
    "    \n",
    "    # Concatenate the remaining processed data list and save to CSV\n",
    "    processed_data_concatenated = pd.concat(processed_data, ignore_index=True)\n",
    "    processed_data_concatenated.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/Users/a/Downloads/Others/恰饭/网址_买官.xlsx'    # Replace with the path to your input Excel file\n",
    "output_file = 'output.csv'   # Replace with the desired path for the output CSV file\n",
    "\n",
    "process_excel(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
